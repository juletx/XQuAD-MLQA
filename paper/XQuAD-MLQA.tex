% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% Custom packages
\usepackage{booktabs}
\usepackage{adjustbox}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Zero-Shot and Translation Experiments on XQuAD, MLQA and TyDiQA}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Julen Etxaniz \\
  UPV/EHU \\
  \texttt{jetxaniz007@ikasle.ehu.eus} \\\And
  Oihane Cantero \\
  UPV/EHU \\
  \texttt{ocantero003@ikasle.ehu.eus}}

\begin{document}
\maketitle
\begin{abstract}

Question answering is a popular task in NLP, that consist in finding the answer of a given question taking a passage of text. There are a lot of models trained to this task on different datasets and languages. In this work, we propose some zero-shot and translation experiments to evaluate these models in three of these datasets (XQuAD, MLQA and TyDiQA). We report the results and make some analysis to see which are the models, languages and experiments that give better results. We see that for low-level resource languages, it is difficult to reach as high results as those we get for the high-level resource ones, because of the lack of data.


\end{abstract}

\section{Introduction}

Question answering (QA) is a popular area in NLP, with many datasets available to tackle the problem from various angles. Despite such popularity, QA datasets in languages other than English remain scarce, even for relatively high-resource languages. The main reason for this is that collecting such datasets at sufficient scale and quality is difficult and costly.

There are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA without relevant benchmark data. Second, we cannot easily train end-to-end QA models on the task, and most recent successes in QA have been in fully supervised settings.

There are a few datasets that try to address the first issue, by providing multilingual validation data. However, training data for multiple languages remains scarce and these datasets include few or no training data. That's why zero-shot and translation settings are popular options to test the performance of models on these datasets.

In this work we perform some zero-shot and translation experiments on 3 multilingual question answering datasets: XQuAD \cite{Artetxe:etal:2019}, MLQA \cite{lewis2019mlqa}, and TyDi QA \cite{tydiqa}. The objective is to compare the results of zero shot, translate-train and translate-test settings on each datasets with different models. We use BERT and RoBERTa models of different sizes and their multilingual versions mBERT and XLM-R.

The QA datasets are described in the following section. Next sections explain the models we used and the experiments we performed. Then we present the baseline results and we discuss our results. Finally, we extract some conclusions.

\section{Datasets}

We perform experiments in 3 multilingual extractive question answering datasets: XQuAD \cite{Artetxe:etal:2019}, MLQA \cite{lewis2019mlqa}, and TyDi QA \cite{tydiqa} with 12, 7, and 9 languages respectively. Each dataset has unique features that justify experimenting with all of them to extarct conclusions.

\subsection{XQuAD}

XQuAD is a multilingual Question Answering dataset \cite{Artetxe:etal:2019}. It is composed of 240 paraghaps and 1190 question-answer pair from SQuAD v1.1\footnote{\url{https://huggingface.co/datasets/squad}}. SQuAD is based on a set of Wikipedia articles in English. Professional translations into 11 languages were added in XQuAD (Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi and Romanian). As the dataset is based on SQuAD v1.1, there are no unanswerable questions in the data. 

We also added automatic translation from XTREME \cite{XTREME} for translate-train and translate-test experiments. The combined dataset can be found in HuggingFace \footnote{\url{https://huggingface.co/datasets/juletxara/xquad_xtreme}}.

\subsection{MLQA}

MLQA \cite{lewis2019mlqa} is another multilingual question answering evaluation benchmark. It has 5K extractive question-answering instances (12K in English) in seven languages (English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese). It is also based on Wikipedia articles, and the questions has been translated by professional translators, while he answers are directly taken form the different languages of the given Wikipedia article, to get parallel sentences. This allows testing settings where context and question languages are different.

Automatic translation from XTREME \cite{XTREME} were also added for translate-train and translate-test experiments. The combined can be found in HuggingFace \footnote{\url{https://huggingface.co/datasets/mlqa}}.

\subsection{TyDiQA}

TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs \cite{tydiqa}. The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language expresses -- such that we expect models performing well on this set to generalize across a large number of the languages in the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but donâ€™t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without the use of translation (unlike MLQA and XQuAD).

We also added automatic translation from XTREME \cite{XTREME} for translate-train and translate-test experiments. The combined can be found in HuggingFace \footnote{\url{https://huggingface.co/datasets/juletxara/tydiqa_xtreme}}.

\section{Models}

We use 7 different models in total for our experiments, 4 monolingual models and three multilingual models.

Monolingual models include the base and large versions of BERT and RoBERTa.

\begin{enumerate}
    \item \textbf{BERT} \cite{BERT}: It is a pretrained model on raw text in English. It has been pretrained with masked language modeling and next sentence prediction objectives to learn an inner representation of the English language. It has 12 layers, the size of the hidden layers is 768, it has 12 self-attention heads and 110M parameters.
    \item \textbf{BERT-large}: It is the large version of BERT and has been pre-trained the same way. It has 24 layers, the size of the hidden layers is 1024, it has 12 self-attention heads and 340M parameters
    \item \textbf{RoBERTa} \cite{ROBERTA}: As the previous models, it has been pretrained on raw English data. It has been trained with dynamic masking, full-sentences without NSP loss, large mini-batches and a larger byte-level BPE. It has 125M parameters, 12 layer of hidden size 768 and 12 attention heads.
    \item \textbf{RoBERTa-large}: It is the large version of RoBERTa. It has 355M parameters, 24 layers of hidden size 1024 and 16 attention heads.
\end{enumerate}

Multilingual models include the multilingual versions of the previous models, mBERT and XLM-R.

\begin{enumerate}
    \item \textbf{mBERT}: It has been trained the same way than BERT but using Wikipedia articles in 102 languages. It has 110M parameters, 12 layers, 768 hidden-states and 12 self-attention heads.
    \item \textbf{XLM-R} \cite{XLM-R}: It has been pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It has 125M parameters with 12 layers, 768 hidden-states, 3072 feed-forward hidden-states, 8 self-attention heads.
    \item \textbf{XLM-R-large}: It is the large version of XLM-R. It has 355M parameters with 24 layers, 1027 hidden-states, 4096 feed-forward hidden-states, and 16 self-attention heads,
\end{enumerate}

\section{Experiments}

We perform 6 experiments in total with the models in the previous section. These experiments include zero-shot, translate-test, translate-train, fine-tuning and data-augmentation. The models we use are already fine-tuned and available on Huggingface. This way we save training time and we can do more experiments. All the code to replicate the experiments can be found on GitHub \footnote{\url{https://github.com/juletx/multilingual-question-answering}}.

\begin{enumerate}
    \item \textbf{Zero-shot}: We fine-tune the multilingual models on SQuAD, and evaluate them on the XQuAD, MLQA and TiDyQA test data for other languages. This is known as cross-lingual zero-shot transfer (XLT). For MLQA, we also evaluate models on generalised cross-lingual zero-shot transfer (G-XLT). In this setting, different languages are used for context and question.
    
    \item \textbf{Translate-test monolingual}: We fine-tune the monolingual models on SQuAD, and evaluate them on translated XQuAD, MLQA and TiDyQA test data.
    
    \item \textbf{Translate-test multilingual}: We fine-tune the multilingual models on SQuAD, and evaluate them on translated XQuAD, MLQA and TiDyQA test data.
    
    \item \textbf{Translate-train}: We fine-tune multilingual models on translated SQuAD, and evaluate them on XQuAD, MLQA and TiDyQA test data.
    
    \item \textbf{Fine-tuning}: For XQuAD dataset, we fine-tune multilingual models on XQuAD and evaluate them on XQuAD test data. As we used already fine-tuned models, we know that the fine-tuning has been done with a part of the test data, but we don't know which one, so the results will be very high, as shown in section \ref{sec:XQuAD_res}. For TiDyQA, the multilingual models are fine-tuned on TiDyQA, and evaluated on TiDyQA test set.
    
    \item \textbf{Data augmentation}: In XQuAD, we fine tune multilingual models on augmented XQuAD, and evaluate them on XQuAD test data. In this case also, we have the same issue than in the previous one, because we don't know in which part of the test data the fine-tuning has been done. For TiDyQA, we use an model that has been trained on augmented XQuAD dataset and then fine-tuned on TyDiQA, and evaluate it on TyDiQA test set.
\end{enumerate}

\section{Baselines}

Each dataset provides a few baseline results as reference. In this section, we comment those results so that we can compare our results with them in the next section.

\subsection{XQuAD}

In Table \ref{Baseline_results_XQuAD}, we can see the baseline results, zero shot results are found by directly fine-tuning mBERT and XLM-R Large in the English SQuAD v1.1 training data and evaluating them on XQuAD test dataset. For translate-train, the models are fine-tuned on translated SQuAD, and for translate-test, BERT Large was fine-tuned on SQuAD training set and evaluated on translated XQuAD.

We can see that the best results are obtained by zero-shot with XLM-Large, and Translate train using BERT Large.

\begin{table*}[!ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|cccccccccccc|c}
    \toprule
        Model & en & ar & de & el & es & hi & ru & th & tr & vi & zh & ro & avg \\ \midrule
        \multicolumn{14}{l}{Zero-shot} \\ \midrule
        mBERT & 83.5 / 72.2 & 61.5 / 45.1 & 70.6 / 54.0 & 62.6 / 44.9 & 75.5 / 56.9 & 59.2 / 46.0 & 71.3 / 53.3 & 42.7 / 33.5 & 55.4 / 40.1 & 69.5 / 49.6 & 58.0 / 48.3 & 72.7 / 59.9 & 65.2 / 50.3 \\ 
        XLM-R Large & 86.5 / 75.7 & 68.6 / 49.0 & \textbf{80.4} / 63.4 & \textbf{79.8} / 61.7 & 82.0 / 63.9 & \textbf{76.7} / 59.7 & \textbf{80.1} / 64.3 & \textbf{74.2} / \textbf{62.8} & \textbf{75.9} / \textbf{59.3} & \textbf{79.1} / 59.0 & 59.3 / 50.0 & \textbf{83.6} / \textbf{69.7} & \textbf{77.2}/ 61.5 \\ \midrule
        \multicolumn{14}{l}{Translate-test} \\ \midrule
        BERT Large & \textbf{87.9} / \textbf{77.1} & \textbf{73.7} / \textbf{58.8} & 79.8 / \textbf{66.7} & 79.4 / \textbf{65.5} & \textbf{82.0} / \textbf{68.4} & 74.9 / \textbf{60.1} & 79.9 / \textbf{66.7} & 64.6 / 50.0 & 67.4 / 49.6 & 76.3 / \textbf{61.5} & \textbf{73.7} / \textbf{59.1} & ~ & 76.3 / \textbf{62.1} \\ \midrule
        \multicolumn{14}{l}{Translate-train} \\ \midrule
        mBERT & 83.5 / 72.2 & 68.0 / 51.1 & 75.6 / 60.7 & 70.0 / 53.0 & 80.2 / 63.1 & 69.6 / 55.4 & 75.0 / 59.7 & 36.9 / 33.5 & 68.9 / 54.8 & 75.6 / 56.2 & 66.2 / 56.6 & ~ & 70.0 / 56.0 \\ 
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Baseline results (F1/EM) for each language in XQuAD dataset.}
    \label{Baseline_results_XQuAD}
\end{table*}


\subsection{MLQA XLT}

The baseline results for MLQA dataset are shown in Table \ref{Baseline_results_MLQA}. XLM performs best overall, transferring best in Spanish, German and Arabic, and competitively with translate-train with mBERT for Vietnamese and Chinese. However, XLM is weaker in English. There is a 39.8\% drop in mean EM score (20.9\% F1) over the English BERT-large baseline,
showing significant room for improvement. All models generally struggle on Arabic and Hindi.

\begin{table*}[!ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|ccccccc|c}
    \toprule
        Model & en & es & de & ar & hi & vi & zh & avg \\ \midrule
        \multicolumn{9}{l}{Zero-shot} \\ \midrule
        BERT-Large & \textbf{80.2} / \textbf{67.4} & ~ & ~ & ~ & ~ & ~ & ~ & 80.2 / 67.4 \\ 
        mBERT & 77.7 / 65.2 & 64.3 / 46.6 & 57.9 / 44.3 & 45.7 / 29.8 & 43.8 / 29.7 & 57.1 / 38.6 & 57.5 / 37.3 & 57.7 / 41.6 \\ 
        XLM & 74.9 / 62.4 & \textbf{68.0} / \textbf{49.8} & \textbf{62.2} / \textbf{47.6} & \textbf{54.8} / \textbf{36.3} & 48.8 / 27.3 & 61.4 / 41.8 & 61.1 / \textbf{39.6} & 61.1 / 43.5 \\ \midrule
        \multicolumn{9}{l}{Translate-test} \\ \midrule
        BERT-Large & ~ & 65.4 / 44.0 & 57.9 / 41.8 & 33.6 / 20.4 & 23.8 / 18.9 & 58.2 / 33.2 & 44.2 / 20.3 & 58.4 / 39.9 \\ \midrule
        \multicolumn{9}{l}{Translate-train} \\ \midrule
        mBERT & ~ & 53.9 / 37.4 & 62.0 / 47.5 & 51.8 / 33.2 & \textbf{55.0} / \textbf{40.0} & \textbf{62.0} / \textbf{43.1} & \textbf{61.4} / 39.5 & 47.2 / 29.7\\
        XLM & ~ & 65.2 / 47.8 & 61.4 / 46.7 & 54.0 / 34.4 & 50.7 / 33.4 & 59.3 / 39.4 & 59.8 / 37.9 & 57.7 / 40.1 \\ 
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Baseline results (F1/EM) for each language in MLQA dataset.}
    \label{Baseline_results_MLQA}
\end{table*}

\subsection{MLQA G-XLT}

Table \ref{tab:mlqa_zero-shot_baseline_xlm} shows results for XLM on G-XLT. For questions in a given language, the model performs best when the context language matches the question, except for Hindi and Arabic. For contexts in a given language, English questions tend to
perform best, apart from Chinese and Vietnamese.

\begin{table}[ht]
\centering
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{l|ccccccc}
\toprule
c/q & en   & es   & de   & ar   & hi   & vi   & zh   \\ \midrule
en  & 74.9 & 65.0 & 58.5 & 50.8 & 43.6 & 55.7 & 53.9 \\
es  & 69.5 & 68.0 & 61.7 & 54.0 & 49.5 & 58.1 & 56.5 \\
de  & 70.6 & 67.7 & 62.2 & 57.4 & 49.9 & 60.1 & 57.3 \\
ar  & 60.0 & 57.8 & 54.9 & 54.8 & 42.4 & 50.5 & 43.5 \\
hi  & 59.6 & 56.3 & 50.5 & 44.4 & 48.8 & 48.9 & 40.2 \\
vi  & 60.2 & 59.6 & 53.2 & 48.7 & 40.5 & 61.4 & 48.5 \\
zh  & 52.9 & 55.8 & 50.0 & 40.9 & 35.4 & 46.5 & 61.1 \\ \bottomrule
\end{tabular}
\end{adjustbox}
\caption{Baseline MLQA F1 results on G-XLT with XLM. Columns show question language, rows show context language.}
\label{tab:mlqa_zero-shot_baseline_xlm}
\end{table}

Table \ref{tab:mlqa_zero-shot_baseline_mbert} shows results for mBERT on  G-XLT. XLM outperforms mBERT for most language pairs, with a mean G-XLT performance of 53.4 F1 compared to 47.2 F1 (mean of off-diagonal elements). Multilingual BERT exhibits more of a preference for English than XLM for G-XLT. It also has a bigger performance drop going from XLT to G-XLT (10.5 mean drop in F1 compared to 8.2).

\begin{table}[ht]
\centering
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{l|ccccccc}
\toprule
c/q & en   & es   & de   & ar   & hi   & vi   & zh   \\ \midrule
en  & 77.7 & 64.4 & 62.7 & 45.7 & 40.1 & 52.2 & 54.2 \\
es  & 67.4 & 64.3 & 58.5 & 44.1 & 38.1 & 48.2 & 51.1 \\
de  & 62.8 & 57.4 & 57.9 & 38.8 & 35.5 & 44.7 & 46.3 \\
ar  & 51.2 & 45.3 & 46.4 & 45.6 & 32.1 & 37.3 & 40.0 \\
hi  & 51.8 & 43.2 & 46.2 & 36.9 & 43.8 & 38.4 & 40.5 \\
vi  & 61.4 & 52.1 & 51.4 & 34.4 & 35.1 & 57.1 & 47.1 \\
zh  & 58.0 & 49.1 & 49.6 & 40.5 & 36.0 & 44.6 & 57.5 \\ \bottomrule
\end{tabular}
\end{adjustbox}
\caption{Baseline MLQA F1 results on G-XLT with mBERT. Columns show question language, rows show context language.}
\label{tab:mlqa_zero-shot_baseline_mbert}
\end{table}

\subsection{TyDiQA GoldP}

Table \ref{TyDiQA_baseline_results} shows baseline results for TyDiQA from \citet{tydiqa}. First, they fine tune mBERT jointly on all languages of the
TyDiQA gold passage training data and evaluate on its dev set. F1 scores remain low, leaving headroom for future improvement.

Second, they fine tune on the 100k English-only
SQuAD 1.1 training set and evaluate on the full
TyDiQA gold passage dev set, following the
XQuAD evaluation zero-shot setting. F1 scores are somewhat lower than the ones observed in the XQuAD zero-shot setting of \citet{Artetxe:etal:2019}. Even the English performance is significantly lower, demonstrating
that the style of question-answer pairs in SQuAD
have very limited value in training a model for
TyDiQA questions.

\begin{table}[!ht]
    \centering
    \begin{adjustbox}{max width=\columnwidth}
    \begin{tabular}{l|ccccccccc|c}
        \toprule
        Model & en & ar & bn & fi & id & ko & ru & sw & te & avg \\
        \midrule
        \multicolumn{11}{l}{Zero-shot} \\
        \midrule
        mBERT & 73.4 & 60.3 & 57.3 & 56.2 & 60.8 & 52.9 & 50.0 & 64.4 & 49.3 & 56.4  \\ \midrule
        \multicolumn{11}{l}{Fine-tuning} \\
        \midrule
        mBERT & 76.8 & 81.7 & 75.4 & 79.4 & 84.8 & 81.9 & 69.2 & 76.2 & 83.3 & 79.0  \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \centering
    \caption{Baseline TyDiQA GoldP F1 results for each language.}
    \label{TyDiQA_baseline_results}
\end{table}

\section{Results}

In this section, we explain our results and compare them with the baseline results from the previous section. We interpret the results of each dataset and extract some conclusions.

\subsection{XQuAD}
\label{sec:XQuAD_res}

The results are obtained with XQuAD dataset are in Table \ref{XQuAD_results}. They are quite similar to those form the baseline and these are some conclusions we got.

We can see that zero-shot is better than translate-test for larger models and worse for smaller models. So we can deduce that larger models have more adaptability to unseen languages than smaller ones. Monolingual models get better results that multilingual ones translate-test, and as we might expect, larger models give better results than smaller ones.

Overall, the results from worst to better have been: Translate-train, Translate-test multilingual, monolingual, zero-shot, data augmentation and fine-tuning. The comparison of the results with fine tuning and data augmentation is not very pertinent because the fine tuning has been done with a part of the testing data. As we don't know which part has been used to fine tune the models, we couldn't remove them from the testing data.

The best languages have been English, Spanish, Romanian and Russian and the worst ones have been Chinese, Hindi, Thai and Turkish, with some very bad results, as for example, 25.2 F1 score and 16.8 EM for fine tuned mBERT in Thai. This could be because these four languages are are not in Latin script, and because Thai is not supported by mBERT.

\begin{table*}[!ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|cccccccccccc|c}
        \toprule
        Model & en & ar & de & el & es & hi & ru & th & tr & vi & zh & ro & avg \\
        \midrule
        \multicolumn{14}{l}{Zero-shot} \\
        \midrule
        mBERT & 85.0 / 73.5 & 57.8 / 42.2 & 72.6 / 55.9 & 62.2 / 45.2 & 76.4 / 58.1 & 55.3 / 40.6 & 71.3 / 54.7 & 35.1 / 26.3 & 51.1 / 34.9 & 68.1 / 47.9 & 58.2 / 47.3 & 72.4 / 59.5 & 63.8 / 48.8 \\
        XLM-R & 84.4 / 73.8 & 67.9 / 52.1 & 75.3 / 59.8 & 74.3 / 57.0 & 77.0 / 59.2 & 69.0 / 52.5 & 75.1 / 58.6 & 68.0 / 56.4 & 68.0 / 51.8 & 73.6 / 54.5 & 65.0 / 55.0 & 80.0 / 66.3 & 73.1 / 58.1 \\
        XLM-R Large &\textbf{86.5 / 75.9}&\textbf{75.0 / 58.0}&\textbf{79.9 / 63.8}&\textbf{79.1 / 61.3}&\textbf{81.0 / 62.7}&\textbf{76.0 / 60.8}&\textbf{80.3 / 63.1}&\textbf{72.8 / 61.7}&\textbf{74.1 / 58.3}&\textbf{79.0 / 59.3}&\textbf{66.8 / 58.0}&\textbf{83.5 / 70.2}&\textbf{77.8 / 62.8}\\
        \midrule
        \multicolumn{14}{l}{Translate-test monolingual} \\
        \midrule
        BERT & ~ & 69.4 / 55.0 & 75.7 / 62.7 & 75.0 / 60.6 & 77.2 / 62.6 & 69.7 / 53.7 & 74.9 / 60.5 & 60.5 / 46.5 & 59.9 / 41.8 & 72.2 / 58.3 & 69.9 / 56.0 & ~ & 70.4 / 55.8 \\
        BERT Large & ~ & 73.6 / 59.1 & 80.4 / 66.4 & 80.2 / 66.8 & 81.9 / 68.7 &\textbf{75.3 / 61.7}& 80.1 / 67.0 &\textbf{67.5 / 53.9}&\textbf{66.3 / 47.3}&\textbf{76.4 / 62.1}& 74.0 / 59.5 & ~ & 75.6 / 61.2 \\
        RoBERTa & ~ & 71.6 / 57.0 & 77.0 / 62.4 & 76.8 / 63.9 & 80.0 / 64.6 & 72.0 / 55.6 & 77.2 / 62.4 & 62.2 / 46.6 & 63.4 / 44.1 & 72.4 / 56.6 & 72.4 / 57.9 & ~ & 72.5 / 57.1 \\
        RoBERTa Large & ~ &\textbf{74.8 / 61.1}&\textbf{80.4 / 67.1}&\textbf{80.8 / 68.0}&\textbf{83.1 / 69.4}& 75.1 / 61.0 &\textbf{81.2 / 68.0}& 65.3 / 51.0 & 66.0 / 46.9 & 76.4 / 62.0 &\textbf{74.0 / 59.9}& ~ &\textbf{75.7 / 61.4}\\
        \midrule
        \multicolumn{14}{l}{Translate-test multilingual} \\
        \midrule
        mBERT & ~ & 70.4 / 55.8 & 76.7 / 63.3 & 76.0 / 61.9 & 78.7 / 65.1 & 70.6 / 55.8 & 76.6 / 63.1 & 60.0 / 45.9 & 61.6 / 42.7 & 70.6 / 55.6 & 70.1 / 56.6 & ~ & 71.2 / 56.6 \\
        XLM-R & ~ & 70.4 / 56.5 & 79.0 / 65.8 & 77.8 / 65.0 & 79.3 / 66.4 & 72.4 / 57.6 & 77.4 / 63.6 & 60.3 / 45.4 & 63.4 / 44.3 & 73.0 / 58.4 & 71.1 / 57.4 & ~ & 72.4 / 58.0 \\
        XLM-R Large & ~ &\textbf{72.9 / 59.1}&\textbf{80.1 / 66.6}&\textbf{79.6 / 66.2}&\textbf{81.5 / 67.1}&\textbf{74.2 / 60.1}&\textbf{79.7 / 65.7}&\textbf{61.7 / 46.0}&\textbf{66.2 / 48.2}&\textbf{75.1 / 61.5}&\textbf{73.6 / 58.8}& ~ &\textbf{74.5 / 59.9}\\
        \midrule
        \multicolumn{14}{l}{Translate-train} \\
        \midrule
        XLM-R-es &\textbf{80.4} / 66.1 &\textbf{67.0} / 47.9 & 74.2 / 56.4 &\textbf{73.5}/ 52.4 &\textbf{76.3} / 56.6 &\textbf{66.9}/ 48.2 & 72.4 / 54.2 &\textbf{68.7} / \textbf{58.5} & \textbf{66.2} / 46.5 & 73.2 / 52.0 & 63.4 / 50.3 &\textbf{76.0} / 59.2 &\textbf{71.5} / 54.0 \\
        XLM-R-de & 79.8 / \textbf{67.1}& 65.9 / \textbf{48.2}&\textbf{74.3} / \textbf{58.8}& 72.3 / \textbf{54.4}& 75.9 / \textbf{57.9}& 66.4 / \textbf{50.6}&\textbf{73.1} / \textbf{56.4}& 65.4 / 56.8 & 65.8 / \textbf{50.8}& 72.7 / \textbf{53.2}& \textbf{64.7} / \textbf{55.0}& 75.3 / \textbf{61.1}& 71.0 / \textbf{55.9}\\
        \midrule
        \multicolumn{14}{l}{Fine-tuning XQuAD} \\
        \midrule
        mBERT & 97.3 / 95.3 & 90.0 / 84.3 & 94.2 / 90.0 & 92.2 / 87.0 & 96.2 / 92.4 & 88.2 / 77.5 & 94.4 / 90.1 & 25.2 / 16.8 & 89.9 / 84.4 & 93.4 / 87.6 & 87.5 / 84.4 & 95.5 / 91.3 & 87.0 / 81.8 \\
        XLM-R & 98.5 / 97.5 & 92.5 / 88.2 & 95.1 / 91.8 & 96.0 / 91.8 & 97.8 / 93.6 & 92.6 / 88.6 & 95.2 / 90.8 & 94.0 / 92.4 & 92.0 / 87.3 & 95.5 / 91.3 & 94.0 / 92.9 & 97.7 / 94.8 & 95.1 / 91.8 \\
        XLM-R Large &\textbf{99.7 / 99.2}&\textbf{97.0 / 94.2}&\textbf{98.1 / 95.6}&\textbf{97.8 / 94.4}&\textbf{98.5 / 95.8}&\textbf{96.5 / 93.6}&\textbf{98.1 / 96.0}&\textbf{96.1 / 95.1}&\textbf{95.9 / 92.3}&\textbf{97.6 / 94.0}&\textbf{96.3 / 95.7}&\textbf{98.9 / 97.1}&\textbf{97.5 / 95.2}\\
        \midrule
        \multicolumn{14}{l}{Data-augmentation XQuAD} \\
        \midrule
        mBERT & 99.7 / 99.2 & 97.1 / 94.4 & 98.9 / 97.9 & 97.0 / 94.6 & 99.6 / 98.9 & 97.7 / 95.1 & 98.5 / 97.3 & 87.3 / 84.9 & 98.8 / 97.4 & 98.9 / 97.5 & 97.5 / 96.8 & 90.6 / 81.6 & 96.8 / 94.6 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \centering
    \caption{XQuAD results (F1/EM) for each language.}
    \label{XQuAD_results}
\end{table*}

\subsection{MLQA XLT}

In the MLQA dataset also, we get higher results with the biggest models, as we can see in Table \ref{MLQA_results}. The language that obtains the best score is English. It is not unexpected because the dataset has much more data in English than in the other languages.
Comparing to the results we got with XQuAD dataset, the results are generally a little lower, but we get the best results with the same models: XLM-R Large for zero-shot and multilingual translate test, RoBERTa Large and BERT Large for monolingual translate test, and balanced between Spanish and German in translate train.

\begin{table*}[!ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|ccccccc|c}
        \toprule
        Model & en & es & de & ar & hi & vi & zh & avg \\
        \midrule
        \multicolumn{9}{l}{Zero-shot} \\
        \midrule
        mBERT & 80.3 / 67.0 & 64.9 / 43.6 & 59.4 / 43.8 & 44.9 / 28.0 & 46.2 / 30.0 & 58.8 / 39.6 & 37.4 / 36.8 & 56.0 / 41.3 \\
        XLM-R & 80.8 / 68.0 & 66.5 / 46.1 & 62.2 / 46.7 & 54.6 / 36.0 & 61.4 / 44.2 & 67.2 / 46.3 & 40.0 / 39.3 & 61.8 / 46.7 \\
        XLM-R Large & \textbf{84.0} / \textbf{71.2} & \textbf{72.1} / \textbf{50.2} & \textbf{68.5} / \textbf{52.4} & \textbf{62.0} / \textbf{42.1} & \textbf{69.8} / \textbf{51.3} & \textbf{73.1} / \textbf{51.8} & \textbf{45.7} / \textbf{45.1} & \textbf{67.9} / \textbf{52.0} \\
        \midrule
        \multicolumn{9}{l}{Translate-test monolingual} \\
        \midrule
        BERT & ~ & 65.0 / 43.2 & 54.4 / 35.7 & 51.0 / 27.7 & 52.8 / 32.0 & 53.6 / 32.1 & 47.8 / 26.6 & 54.1 / 32.9 \\
        BERT Large & ~ & 67.2 / 45.2 & 56.7 / 37.2 & 52.7 / 28.9 & 55.2 / 33.8 & \textbf{56.7} / 34.7 & 50.1 / \textbf{27.8} & 56.4 / 34.6 \\
        RoBERTa & ~ & 66.0 / 43.4 & 54.1 / 34.1 & 51.4 / 27.6 & 52.3 / 31.0 & 54.0 / 32.4 & 47.6 / 25.2 & 54.3 / 32.3 \\
        RoBERTa Large & ~ &\textbf{68.0} /\textbf{45.9} & \textbf{57.4} / \textbf{38.0} & \textbf{53.7} / \textbf{29.4} & \textbf{55.7} / \textbf{33.9} & 56.3 / \textbf{34.9} & \textbf{50.6} / 27.7 & \textbf{56.9} / \textbf{35.0}\\
        \midrule
        \multicolumn{9}{l}{Translate-test multilingual} \\
        \midrule
        mBERT & ~ & 64.3 / 43.0 & 53.6 / 34.8 & 49.5 / 27.0 & 51.9 / 31.2 & 53.4 / 32.0 & 45.9 / 24.5 & 53.1 / 32.1 \\
        XLM-R & ~ &64.8 / 43.0 & 53.6 / 34.9 & 50.4 / 27.7 & 52.8 / 32.0 & 54.2 / 33.4 & 47.7 / 26.1 & 53.9 / 32.9 \\
        XLM-R Large & ~ & \textbf{68.6} / \textbf{46.5} & \textbf{56.6} / \textbf{37.4} & \textbf{53.1} / \textbf{29.2} & \textbf{55.6} / \textbf{34.5} & \textbf{56.6}/ \textbf{34.5} & \textbf{50.0} / \textbf{27.6} & \textbf{56.7} / \textbf{35.0} \\
        \midrule
        \multicolumn{9}{l}{Translate-train} \\
        \midrule
        XLM-R-es &77.2 / 61.5 & \textbf{68.0} / 44.8 & 61.4 / 44.9 & \textbf{54.1} / 34.1 & \textbf{60.2} / 40.7 & \textbf{66.2} / 45.0 & 36.2 / 35.4 & \textbf{60.5} / 43.8 \\
        XLM-R-de & \textbf{77.3} / \textbf{63.6} & 65.6 / \textbf{45.0} & \textbf{62.4} / \textbf{46.7} & 53.6 / \textbf{35.6} & 60.1 / \textbf{43.8} & 65.0 / \textbf{45.2} & \textbf{38.1} / \textbf{37.4} & 60.3 / \textbf{45.3}\\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \centering
    \caption{MLQA results (F1/EM) for each language.}
    \label{MLQA_results}
\end{table*}

\subsection{MLQA G-XLT}

We made zero-shot experiments between all the seven languages of the MLQA dataset and here are the results we got with the three models we used.

Using mBERT, in Table \ref{MLQA_results_ZS_mBERT} we see that no matter the language of the corpus, making the question in English always gives the best score. This is probably because the dataset is trained with more data in English than the other languages. In the cases of Spanish, German, Vietnamese and Chinese, the second best score corresponds to the case where the question is asked in the language itself, but for Arabic and Hindi, we get better results when the question language is Spanish or German, instead of Arabic or Hindi.

\begin{table*}[!ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|ccccccc|c}
    \toprule
        c/q & en & es & de & ar & hi & vi & zh & avg \\ \midrule
        en & \textbf{80.3} / \textbf{67.0} & 67.4 / 52.8 & 66.4 / 52.5 & 44.1 / 31.1 & 39.3 / 26.3 & 53.7 / 39.1 & 55.8 / 41.4 & 58.1 / 44.3 \\
        es & \textbf{66.9} /\textbf{46.4} & 64.9 / 43.6 & 60.6 / 40.2 & 43.1 / 26.0 & 36.2 / 20.1 & 48.5 / 31.4 & 49.9 / 30.6 & 52.9 / 34.0 \\ 
        de & \textbf{62.4} / \textbf{46.7} & 56.4 / 41.0 & 59.4 / 43.8 & 36.8 / 23.6 & 34.0 / 21.5 & 43.6 / 29.6 & 46.5 / 30.7 & 48.4 / 33.8 \\ 
        ar & \textbf{51.1} / \textbf{33.7} & 45.4 / 28.7 & 46.3 / 30.5 & 44.9 / 28.0 & 30.8 / 17.3 & 35.9 / 20.1 & 36.8 / 21.3 & 41.6 / 25.7 \\ 
        hi & \textbf{52.9} / \textbf{37.1} & 43.7 / 29.1 & 47.6 / 33.8 & 34.5 / 21.4 & 46.2 / 30.0 & 38.0 / 25.0 & 39.2 / 25.2 & 43.2 / 28.8 \\ 
        vi & \textbf{64.5} / \textbf{44.8} & 53.9 / 37.5 & 53.7 / 36.6 & 32.5 / 19.3 & 35.1 / 19.7 & 25.8 / 39.6 & 50.3 / 32.3 & 49.8 / 32.8 \\ 
        zh & \textbf{38.3} / \textbf{37.7} & 29.0 / 28.3 & 30.0 / 28.9 & 21.0 / 20.6 & 16.6 / 16.2 & 25.1 / 24.4 & 37.4 / 36.8 & 28.2 / 27.6 \\ \midrule
        avg & 59.5 / 44.8 & 51.5 / 37.3 & 52.0 / 38.0 & 36.7 / 24.3 & 34.0 / 21.6 & 43.4 / 29.9 & 45.1 / 31.2 & 46.0 / 32.4 \\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{MLQA results (F1/EM) for each language in zero-shot with mBERT. Columns show question language, rows show context language.}
    \label{MLQA_results_ZS_mBERT}
\end{table*}

Using XLM-R, we see in Table \ref{MLQA_results_ZS_XLM-R} that the best scores are always those that have the same question and context language. 

\begin{table*}[ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|ccccccc|c}
    \toprule
        c/q & en & es & de & ar & hi & vi & zh & avg \\ \midrule
        en & \textbf{80.8} / \textbf{68.0} & 57.8 / 43.9 & 60.8 / 47.1 & 33.5 / 21.3 & 45.0 / 32.0 & 39.8 / 27.5 & 37.9 / 25.3 & 50.8 / 37.9 \\ 
        es & 66.0 / 45.1 & \textbf{66.5} / \textbf{46.1} & 50.5 / 32.6 & 25.2 / 12.3 & 31.8 / 17.1 & 29.1 / 14.9 & 28.2 / 14.3 & 42.5 / 26.1 \\ 
        de & 60.0 / 44.3 & 44.0 / 29.7 & \textbf{62.2} / \textbf{46.7} & 22.2 / 12.1 & 29.4 / 17.6 & 28.7 / 16.2 & 29.1 / 17.2 & 39.4 / 26.3 \\ 
        ar & 51.5 / 33.8 & 27.0 / 13.5 & 34.2 / 19.8 & \textbf{54.6} / \textbf{36.0} & 15.6 / 5.8 & 15.0 / 5.7 & 14.1 / 5.1 & 30.3 / 17.1 \\ 
        hi & 60.6 / 43.4 & 37.4 / 23.0 & 42.8 / 27.8 & 19.5 / 8.0 & \textbf{61.4} / \textbf{44.2} & 24.3 / 11.9 & 26.1 / 13.6 & 38.9 / 24.6 \\ 
        vi & 63.6 / 44.6 & 32.6 / 19.1 & 41.9 / 25.7 & 17.8 / 6.6 & 29.2 / 15.0 & \textbf{67.2} / \textbf{46.3} & 27.4 / 13.8 & 40.0 / 24.4 \\ 
        zh & 34.9 / 34.3 & 11.3 / 10.7 & 14.0 / 13.3 & 3.9 / 3.7 & 10.8 / 10.4 & 8.1 / 7.7 & \textbf{40.0} / \textbf{39.3} & 17.6 / 17.1 \\ \midrule
        avg & 59.6 / 44.8 & 39.5 / 26.6 & 43.8 / 30.4 & 25.2 / 14.3 & 31.9 / 20.3 & 30.3 / 18.6 & 29.0 / 18.4 & 37.0 / 24.8 \\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{MLQA results (F1/EM) for each language in zero-shot with XLM-R. Columns show question language, rows show context language.}
    \label{MLQA_results_ZS_XLM-R}
\end{table*}


As we can see in Table \ref{MLQA_results_ZS_XLM-R-Large}, here also in most of the cases the best scores are when the question and the context are in the same language, even if the English scores are very close, and in some cases EM is better in English.

\begin{table*}[ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|ccccccc|c}
    \toprule
        c/q & en & es & de & ar & hi & vi & zh & avg \\ \midrule
        en & \textbf{84.0} / \textbf{71.2} & 77.2 / 64.2 & 77.7 / 65.1 & 32.4 / 22.1 & 43.6 / 30.7 & 61.6 / 48.5 & 33.8 / 21.1 & 58.6 / 46.1 \\ 
        es & \textbf{72.1} / \textbf{50.3} & \textbf{72.1} / 50.2 & 70.0 / 48.6 & 33.0 / 17.8 & 42.1 / 26.1 & 54.8 / 35.6 & 36.5 / 20.5 & 54.4 / 35.6 \\ 
        de & 67.7 / 51.7 & 65.3 / 49.6 & \textbf{68.5} / \textbf{52.4} & 31.2 / 19.5 & 36.2 / 21.9 & 50.7 / 34.3 & 32.3 / 18.9 & 50.3 / 35.5 \\ 
        ar & 61.7 / \textbf{42.2} & 56.7 / 38.4 & 59.7 / 41.8 & \textbf{62.0} / \textbf{42.1} & 43.9 / 27.4 & 48.6 / 30.7 & 38.7 / 21.6 & 53.0 / 34.9 \\ 
        hi & 70.5 / \textbf{52.6} & 63.2 / 45.9 & 65.1 / 49.9 & 45.5 / 29.1 & \textbf{69.8} / \textbf{51.3} & 54.4 / 37.6 & 44.6 / 28.5 & 59.0 / 42.1 \\ 
        vi & 72.1 / 50.9 & 64.7 / 45.5 & 67.7 / 48.2 & 35.8 / 20.9 & 42.0 / 25.3 & \textbf{73.1} / \textbf{51.8} & 39.2 / 21.7 & 56.4 / 37.8 \\ 
        zh & 44.2 / 43.6 & 36.7 / 36.1 & 41.1 / 40.2 & 25.9 / 25.4 & 30.0 / 29.5 & 35.0 / 34.6 & \textbf{45.7} / \textbf{45.1} & 36.9 / 36.4 \\ \midrule
        avg & 67.5 / 51.8 & 62.3 / 47.1 & 64.3 / 49.5 & 38.0 / 25.3 & 43.9 / 30.3 & 54.0 / 39.0 & 38.7 / 25.3 & 52.7 / 38.3 \\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{MLQA results (F1/EM) for each language in zero-shot with XLM-R-Large. Columns show question language, rows show context language.}
    \label{MLQA_results_ZS_XLM-R-Large}
\end{table*}

\subsection{TyDiQA GoldP}

In Table \ref{TyDiQA_results}, we can see the results we get with TyDiQA dataset. As in the previous datasets, for zero-shot, the best results are obtained with the larger model, XLM-R large, for every language. For monolingual translate test, a large model also get the best scores, but it is BERT Large instead of RoBERTa Large. For multilingual translate test, XLM-R Large get almost every best results except for Korean en Finnish EM, where mBERT performs better. And for translate train, the results are here also balanced between Spanish and German.  Fine-tuning improves the results in most of the languages of the dataset. And for data augmentation, we get better results than in the rest of the experiments with this dataset, but a little lower than with XQuAD. Taking the case of Arabic, that appears both in MLQA and TyDiQA, we get better results with TyDiQA in every experiment.

\begin{table*}[!ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|ccccccccc|c}
        \toprule
        Model & en & ar & bn & fi & id & ko & ru & sw & te & avg \\
        \midrule
        \multicolumn{11}{l}{Zero-shot} \\
        \midrule
        mBERT & 77.8 / 69.8 & 60.6 / 45.1 & 59.5 / 47.8 & 60.6 / 50.1 & 63.0 / 50.6 & 47.9 / 39.5 & 65.3 / 47.3 & 61.0 / 49.5 & 48.9 / 41.4 & 60.5 / 49.0  \\
        XLM-R & 75.2 / 65.9 & 66.7 / 52.8 & 67.5 / 51.3 & 72.6 / 62.3 & 75.8 / 61.6 & 62.6 / 53.6 & 67.6 / 48.9 & 68.8 / 59.1 & 74.6 / 58.6 & 70.2 / 57.1  \\
        XLM-R Large & \textbf{81.4} / \textbf{70.9} & \textbf{78.2} / \textbf{64.1} & \textbf{75.3} / \textbf{60.2} & \textbf{79.8} / \textbf{68.5} & \textbf{81.7} / \textbf{68.7} & \textbf{72.9} / \textbf{62.0} & \textbf{73.1} / \textbf{52.6} & \textbf{81.6} / \textbf{71.9} & \textbf{80.8} / \textbf{67.1} & \textbf{78.3} / \textbf{65.1}\\
        \midrule
        \multicolumn{11}{l}{Translate-test monolingual} \\
        \midrule
        BERT & ~ & 67.2 / 49.8 & 76.6 / \textbf{64.1} & 71.2 / 57.5 & 74.2 / 60.3 & 70.1 / 58.9 & 71.9 / 55.5 & 76.0 / 63.8 & 63.5 / 51.1 & 71.3 / 57.6 \\
        BERT Large & ~ & \textbf{69.8} / \textbf{52.2} & \textbf{79.0} / \textbf{64.1} & \textbf{76.8} / \textbf{64.2} & \textbf{76.0} / \textbf{62.4} & \textbf{73.8} / \textbf{63.5} & \textbf{75.3} / \textbf{60.1} & \textbf{80.9} / \textbf{69.8} & \textbf{79.7} / \textbf{66.2} & \textbf{76.4} / \textbf{62.8} \\
        RoBERTa & ~ & 66.9 / 47.6 & 72.4 / 57.5 & 74.3 / 60.4 & 74.4 / 60.9 & 70.3 / 57.4 & 71.6 / 55.1 & 78.6 / 67.0 & 69.2 / 55.5 & 72.2 / 57.7 \\
        RoBERTa Large & ~ & 66.9 / 48.1 & 78.1 / 63.0 & 75.4 / 60.6 & 73.0 / 56.8 & 73.2 / 61.6 & 74.3 / 58.1 & 80.2 / 69.6 & 78.0 / 63.2 & 74.9 / 60.1\\
        \midrule
        \multicolumn{11}{l}{Translate-test multilingual} \\
        \midrule
        mBERT & ~ & 66.7 / 48.9 & 70.4 / 56.4 & 73.2 / \textbf{61.6} & 72.9 / 59.1 & \textbf{71.9} / \textbf{60.3} & 72.0 / 55.5 & 79.8 / 68.4 & 66.9 / 55.0 & 71.7 / 58.2 \\
        XLM-R & ~ & 63.6 / 46.4 & 72.4 / 60.8 & 69.6 / 56.9 & 71.2 / 58.3 & 69.6 / 56.2 & 70.8 / 55.1 & 78.8 / 68.9 & 59.4 / 46.8 & 69.4 / 56.2 \\
        XLM-R Large & ~ & \textbf{68.6} / \textbf{52.5} & \textbf{73.3} / \textbf{58.0} & \textbf{75.2} / 61.3 & \textbf{75.5} / \textbf{62.9} & 68.5 / 56.7 & \textbf{73.8} / \textbf{58.7} & \textbf{80.2} / \textbf{69.5} & \textbf{76.6} / \textbf{61.8} & \textbf{74.0} / \textbf{60.2}\\
        \midrule
        \multicolumn{11}{l}{Translate-train} \\
        \midrule
        XLM-R-es & 71.8 / 59.1 & \textbf{68.2} / \textbf{52.1} & 63.6 / 44.2 & 71.2 / 56.3 & \textbf{73.1} / 57.4 & 53.8 / 40.6 & 67.2 / 43.6 & 67.2 / 55.9 & 71.7 / \textbf{54.4} & 67.5 / 51.5 \\
        XLM-R-de & \textbf{73.6} / \textbf{63.2} & 66.0 / 50.2 & \textbf{64.7} / \textbf{49.6} & \textbf{72.4} / \textbf{60.1} & 72.4 / \textbf{60.2} & \textbf{58.2} / \textbf{44.9} & \textbf{68.2} / \textbf{51.6} & \textbf{72.1} / \textbf{63.5} & \textbf{72.8} / 53.8 & \textbf{68.9} / \textbf{55.2} \\
        \midrule
        \multicolumn{11}{l}{Fine-tuning} \\
        \midrule
        mBERT & 74.7 / 63.4 & 81.3 / 68.1 & 65.3 / 54.0 & 79.6 / 69.2 & 81.9 / 70.4 & 63.0 / 52.9 & 71.2 / 60.8 & 81.5 / 75.2 & 80.4 / 66.8 & 75.4 / 64.5  \\
        \midrule
        \multicolumn{11}{l}{Data-augmentation} \\
        \midrule
        mBERT & 84.2 / 74.1 & 86.4 / 74.7 & 77.6 / 65.5 & 84.2 / 74.2 & 88.5 / 80.2 & 75.2 / 67.4 & 81.0 / 70.1 & 85.5 / 79.8 & 84.8 / 71.6 & 83.0 / 73.0 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \centering
    \caption{TyDiQA GoldP results (F1/EM) for each language.}
    \label{TyDiQA_results}
\end{table*}


\section{Conclusions}

Using three different multilingual question answering datasets, and making some experiments with different datasets and models, we saw that overall, the models that work better in both zero-shot and translation experiments (translate test monolingual, multilingual and translate train) are the biggest ones. But for some languages there is a gap in the results depending on the model we use, even if the model is multilingual. We saw that comparing the scores of the different languages of the datasets, most often the best results are obtained by English no matter the model we use and the experiment in which we are. This is because it is the language that has the most resources and biggest clean datasets.

As expected, fine-tuning give better results in some cases and data augmentation improves the results in most of the tries, even if it is not the case with all the languages.



\bibliography{XQuAD-MLQA}
\bibliographystyle{acl_natbib}

\end{document}
