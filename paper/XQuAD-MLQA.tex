% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% Custom packages
\usepackage{booktabs}
\usepackage{adjustbox}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Zero-Shot and Translation Experiments on XQuAD, MLQA and TyDiQA}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Julen Etxaniz \\
  UPV/EHU \\
  \texttt{jetxaniz007@ikasle.ehu.eus} \\\And
  Oihane Cantero \\
  UPV/EHU \\
  \texttt{ocantero003@ikasle.ehu.eus}}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}

\section{Introduction}

In this project we performed some zero-shot and translation experiments on Multilingual Question Answering. The objective is to compare the results of zero shot, translation test and translation test on different datasets, with different models. The datasets we used are XQuAD, MLQA and TyDiQA, and the models are monolingual or multilingual:

\begin{itemize}
    \item Monolingual models: 
    \begin{enumerate}
        \item BERT (110M)
        \item BERT-large (340M)
        \item RoBERTa
        \item RoBERTa-large
    \end{enumerate}
    \item Multilingual models
    \begin{enumerate}
        \item mBERT (110M)
        \item XLM-R
        \item XLM-R-large
    \end{enumerate}
\end{itemize}

Most of the models we used are already fine-tuned and available on Huggingface.

\section{Related Work}

\section{Data}

\subsection{XQuAD}

XQuAD is a multilingual Question Answering dataset \cite{Artetxe:etal:2019}. It is composed of 240 paraghaps and 1190 question-answer pair from SQuAD v1.1\footnote{\url{https://huggingface.co/datasets/squad}}. SQuAD is based on a set of Wikipedia articles. Professional translations into 11 languages were added in XQuAD (Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi and Romanian). As the dataset is based on SQuAD v1.1, there are no unanswerable questions in the data. 

We also used XTREME \cite{XTREME} for automatically translated translate-train and translate-test data. The dataset can be found in HuggingFace \footnote{\url{https://huggingface.co/datasets/juletxara/xquad_xtreme}}.

\subsection{MLQA}

MLQA \cite{lewis2019mlqa} is another multilingual question answering evaluation benchmark. It has 5K extractive question-answering instances (12K in English) in seven languages (English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese). It is also based on Wikipedia articles, and the questions has been translated by professional translators, while he answers are directly taken form the different languages of the given Wikipedia article, to get parallel sentences.

We also used XTREME \cite{XTREME} for automatically translated translate-train and translate-test data. The dataset can be found in HuggingFace \footnote{\url{https://huggingface.co/datasets/mlqa}}.

\subsection{TyDiQA}

TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs \cite{tydiqa}. The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language expresses -- such that we expect models performing well on this set to generalize across a large number of the languages in the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but donâ€™t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without the use of translation (unlike MLQA and XQuAD).

We also used XTREME \cite{XTREME} for automatically translated translate-train and translate-test data. The dataset can be found in HuggingFace \footnote{\url{https://huggingface.co/datasets/juletxara/tydiqa_xtreme}}.

\section{Methods}

All the code can be found on GitHub \footnote{\url{https://github.com/juletx/XQuAD-MLQA}}

\subsection{Zero-shot}


\subsection{Translate Train}

\subsection{Translate Test Monolingual}

\subsection{Translate Test Multilingual}

\subsection{Fine tuning}

\subsection{Data augmentation}

\section{Results}

\subsection{XQuAD}

The results are obtained with XQuAD dataset are in Table \ref{XQuAD_results}. They are quite similar to those form the baseline and these are some conclusions we got.

We can see that zero-shot is better than translate-test for larger models and worse for smaller models. So we can deduce that larger models have more adaptability to unseen languages than smaller ones. Monolingual models get better results that multilingual ones translate-test, and as we might expect, larger models give better results than smaller ones.

Overall, the results from worst to better have been: Translate-train, Translate-test multilingual, monolingual, zero-shot, data augmentation and fine-tuning. The comparison of the results with fine tuning and data augmentation is not very pertinent because the fine tuning has been done with a part of the testing data. As we don't know which part has been used to fine tune the models, we couldn't remove them from the testing data.

The best languages have been English, Spanish, Romanian and Russian and the worst ones have been Chinese, Hindi, Thai and Turkish, with some very bad results, as for example, 25.2 F1 score and 16.8 EM for fine tuned mBERT in Thai. This could be because these four languages are are not in Latin script, and because Thai is not supported by mBERT.

\begin{table*}[!ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|cccccccccccc|c}
        \toprule
        Model F1 / EM & en & ar & de & el & es & hi & ru & th & tr & vi & zh & ro & avg \\
        \midrule
        \multicolumn{14}{l}{Zero-shot} \\
        \midrule
        mBERT & 85.0 / 73.5 & 57.8 / 42.2 & 72.6 / 55.9 & 62.2 / 45.2 & 76.4 / 58.1 & 55.3 / 40.6 & 71.3 / 54.7 & 35.1 / 26.3 & 51.1 / 34.9 & 68.1 / 47.9 & 58.2 / 47.3 & 72.4 / 59.5 & 63.8 / 48.8 \\
        XLM-R & 84.4 / 73.8 & 67.9 / 52.1 & 75.3 / 59.8 & 74.3 / 57.0 & 77.0 / 59.2 & 69.0 / 52.5 & 75.1 / 58.6 & 68.0 / 56.4 & 68.0 / 51.8 & 73.6 / 54.5 & 65.0 / 55.0 & 80.0 / 66.3 & 73.1 / 58.1 \\
        XLM-R Large &\textbf{86.5 / 75.9}&\textbf{75.0 / 58.0}&\textbf{79.9 / 63.8}&\textbf{79.1 / 61.3}&\textbf{81.0 / 62.7}&\textbf{76.0 / 60.8}&\textbf{80.3 / 63.1}&\textbf{72.8 / 61.7}&\textbf{74.1 / 58.3}&\textbf{79.0 / 59.3}&\textbf{66.8 / 58.0}&\textbf{83.5 / 70.2}&\textbf{77.8 / 62.8}\\
        \midrule
        \multicolumn{14}{l}{Translate-test monolingual} \\
        \midrule
        BERT & ~ & 69.4 / 55.0 & 75.7 / 62.7 & 75.0 / 60.6 & 77.2 / 62.6 & 69.7 / 53.7 & 74.9 / 60.5 & 60.5 / 46.5 & 59.9 / 41.8 & 72.2 / 58.3 & 69.9 / 56.0 & ~ & 70.4 / 55.8 \\
        BERT Large & ~ & 73.6 / 59.1 & 80.4 / 66.4 & 80.2 / 66.8 & 81.9 / 68.7 &\textbf{75.3 / 61.7}& 80.1 / 67.0 &\textbf{67.5 / 53.9}&\textbf{66.3 / 47.3}&\textbf{76.4 / 62.1}& 74.0 / 59.5 & ~ & 75.6 / 61.2 \\
        RoBERTa & ~ & 71.6 / 57.0 & 77.0 / 62.4 & 76.8 / 63.9 & 80.0 / 64.6 & 72.0 / 55.6 & 77.2 / 62.4 & 62.2 / 46.6 & 63.4 / 44.1 & 72.4 / 56.6 & 72.4 / 57.9 & ~ & 72.5 / 57.1 \\
        RoBERTa Large & ~ &\textbf{74.8 / 61.1}&\textbf{80.4 / 67.1}&\textbf{80.8 / 68.0}&\textbf{83.1 / 69.4}& 75.1 / 61.0 &\textbf{81.2 / 68.0}& 65.3 / 51.0 & 66.0 / 46.9 & 76.4 / 62.0 &\textbf{74.0 / 59.9}& ~ &\textbf{75.7 / 61.4}\\
        \midrule
        \multicolumn{14}{l}{Translate-test multilingual} \\
        \midrule
        mBERT & ~ & 70.4 / 55.8 & 76.7 / 63.3 & 76.0 / 61.9 & 78.7 / 65.1 & 70.6 / 55.8 & 76.6 / 63.1 & 60.0 / 45.9 & 61.6 / 42.7 & 70.6 / 55.6 & 70.1 / 56.6 & ~ & 71.2 / 56.6 \\
        XLM-R & ~ & 70.4 / 56.5 & 79.0 / 65.8 & 77.8 / 65.0 & 79.3 / 66.4 & 72.4 / 57.6 & 77.4 / 63.6 & 60.3 / 45.4 & 63.4 / 44.3 & 73.0 / 58.4 & 71.1 / 57.4 & ~ & 72.4 / 58.0 \\
        XLM-R Large & ~ &\textbf{72.9 / 59.1}&\textbf{80.1 / 66.6}&\textbf{79.6 / 66.2}&\textbf{81.5 / 67.1}&\textbf{74.2 / 60.1}&\textbf{79.7 / 65.7}&\textbf{61.7 / 46.0}&\textbf{66.2 / 48.2}&\textbf{75.1 / 61.5}&\textbf{73.6 / 58.8}& ~ &\textbf{74.5 / 59.9}\\
        \midrule
        \multicolumn{14}{l}{Translate-train} \\
        \midrule
        XLM-R-es &\textbf{80.4} / 66.1 &\textbf{67.0} / 47.9 & 74.2 / 56.4 &\textbf{73.5}/ 52.4 &\textbf{76.3} / 56.6 &\textbf{66.9}/ 48.2 & 72.4 / 54.2 &\textbf{68.7} / \textbf{58.5} & \textbf{66.2} / 46.5 & 73.2 / 52.0 & 63.4 / 50.3 &\textbf{76.0} / 59.2 &\textbf{71.5} / 54.0 \\
        XLM-R-de & 79.8 / \textbf{67.1}& 65.9 / \textbf{48.2}&\textbf{74.3} / \textbf{58.8}& 72.3 / \textbf{54.4}& 75.9 / \textbf{57.9}& 66.4 / \textbf{50.6}&\textbf{73.1} / \textbf{56.4}& 65.4 / 56.8 & 65.8 / \textbf{50.8}& 72.7 / \textbf{53.2}& \textbf{64.7} / \textbf{55.0}& 75.3 / \textbf{61.1}& 71.0 / \textbf{55.9}\\
        \midrule
        \multicolumn{14}{l}{Fine-tuning XQuAD} \\
        \midrule
        mBERT & 97.3 / 95.3 & 90.0 / 84.3 & 94.2 / 90.0 & 92.2 / 87.0 & 96.2 / 92.4 & 88.2 / 77.5 & 94.4 / 90.1 & 25.2 / 16.8 & 89.9 / 84.4 & 93.4 / 87.6 & 87.5 / 84.4 & 95.5 / 91.3 & 87.0 / 81.8 \\
        XLM-R & 98.5 / 97.5 & 92.5 / 88.2 & 95.1 / 91.8 & 96.0 / 91.8 & 97.8 / 93.6 & 92.6 / 88.6 & 95.2 / 90.8 & 94.0 / 92.4 & 92.0 / 87.3 & 95.5 / 91.3 & 94.0 / 92.9 & 97.7 / 94.8 & 95.1 / 91.8 \\
        XLM-R Large &\textbf{99.7 / 99.2}&\textbf{97.0 / 94.2}&\textbf{98.1 / 95.6}&\textbf{97.8 / 94.4}&\textbf{98.5 / 95.8}&\textbf{96.5 / 93.6}&\textbf{98.1 / 96.0}&\textbf{96.1 / 95.1}&\textbf{95.9 / 92.3}&\textbf{97.6 / 94.0}&\textbf{96.3 / 95.7}&\textbf{98.9 / 97.1}&\textbf{97.5 / 95.2}\\
        \midrule
        \multicolumn{14}{l}{Data-augmentation XQuAD} \\
        \midrule
        mBERT & 99.7 / 99.2 & 97.1 / 94.4 & 98.9 / 97.9 & 97.0 / 94.6 & 99.6 / 98.9 & 97.7 / 95.1 & 98.5 / 97.3 & 87.3 / 84.9 & 98.8 / 97.4 & 98.9 / 97.5 & 97.5 / 96.8 & 90.6 / 81.6 & 96.8 / 94.6 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \centering
    \caption{XQuAD results (F1/EM) for each language.}
    \label{XQuAD_results}
\end{table*}

\subsection{MLQA}

In the MLQA dataset also, we get higher results with the biggest models, as we can see in Table \ref{MLQA_results}. The language that obtains the best score is English. It is not unexpected because the dataset has much more data in English than in the other languages.
Comparing to the results we got with XQuAD dataset, the results are generally a little lower, but we get the best results with the same models: XLM-R Large for zero-shot and multilingual translate test, RoBERTa Large and BERT Large for monolingual translate test, and balanced between Spanish and German in translate train.

\begin{table*}[!ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|ccccccc|c}
        \toprule
        Model F1 / EM & en & es & de & ar & hi & vi & zh & avg \\
        \midrule
        \multicolumn{9}{l}{Zero-shot} \\
        \midrule
        mBERT & 80.3 / 67.0 & 64.9 / 43.6 & 59.4 / 43.8 & 44.9 / 28.0 & 46.2 / 30.0 & 58.8 / 39.6 & 37.4 / 36.8 & 56.0 / 41.3 \\
        XLM-R & 80.8 / 68.0 & 66.5 / 46.1 & 62.2 / 46.7 & 54.6 / 36.0 & 61.4 / 44.2 & 67.2 / 46.3 & 40.0 / 39.3 & 61.8 / 46.7 \\
        XLM-R Large & \textbf{84.0} / \textbf{71.2} & \textbf{72.1} / \textbf{50.2} & \textbf{68.5} / \textbf{52.4} & \textbf{62.0} / \textbf{42.1} & \textbf{69.8} / \textbf{51.3} & \textbf{73.1} / \textbf{51.8} & \textbf{45.7} / \textbf{45.1} & \textbf{67.9} / \textbf{52.0} \\
        \midrule
        \multicolumn{9}{l}{Translate-test monolingual} \\
        \midrule
        BERT & ~ & 65.0 / 43.2 & 54.4 / 35.7 & 51.0 / 27.7 & 52.8 / 32.0 & 53.6 / 32.1 & 47.8 / 26.6 & 54.1 / 32.9 \\
        BERT Large & ~ & 67.2 / 45.2 & 56.7 / 37.2 & 52.7 / 28.9 & 55.2 / 33.8 & \textbf{56.7} / 34.7 & 50.1 / \textbf{27.8} & 56.4 / 34.6 \\
        RoBERTa & ~ & 66.0 / 43.4 & 54.1 / 34.1 & 51.4 / 27.6 & 52.3 / 31.0 & 54.0 / 32.4 & 47.6 / 25.2 & 54.3 / 32.3 \\
        RoBERTa Large & ~ &\textbf{68.0} /\textbf{45.9} & \textbf{57.4} / \textbf{38.0} & \textbf{53.7} / \textbf{29.4} & \textbf{55.7} / \textbf{33.9} & 56.3 / \textbf{34.9} & \textbf{50.6} / 27.7 & \textbf{56.9} / \textbf{35.0}\\
        \midrule
        \multicolumn{9}{l}{Translate-test multilingual} \\
        \midrule
        mBERT & ~ & 64.3 / 43.0 & 53.6 / 34.8 & 49.5 / 27.0 & 51.9 / 31.2 & 53.4 / 32.0 & 45.9 / 24.5 & 53.1 / 32.1 \\
        XLM-R & ~ &64.8 / 43.0 & 53.6 / 34.9 & 50.4 / 27.7 & 52.8 / 32.0 & 54.2 / 33.4 & 47.7 / 26.1 & 53.9 / 32.9 \\
        XLM-R Large & ~ & \textbf{68.6} / \textbf{46.5} & \textbf{56.6} / \textbf{37.4} & \textbf{53.1} / \textbf{29.2} & \textbf{55.6} / \textbf{34.5} & \textbf{56.6}/ \textbf{34.5} & \textbf{50.0} / \textbf{27.6} & \textbf{56.7} / \textbf{35.0} \\
        \midrule
        \multicolumn{9}{l}{Translate-train} \\
        \midrule
        XLM-R-es &77.2 / 61.5 & \textbf{68.0} / 44.8 & 61.4 / 44.9 & \textbf{54.1} / 34.1 & \textbf{60.2} / 40.7 & \textbf{66.2} / 45.0 & 36.2 / 35.4 & \textbf{60.5} / 43.8 \\
        XLM-R-de & \textbf{77.3} / \textbf{63.6} & 65.6 / \textbf{45.0} & \textbf{62.4} / \textbf{46.7} & 53.6 / \textbf{35.6} & 60.1 / \textbf{43.8} & 65.0 / \textbf{45.2} & \textbf{38.1} / \textbf{37.4} & 60.3 / \textbf{45.3}\\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \centering
    \caption{MLQA results (F1/EM) for each language.}
    \label{MLQA_results}
\end{table*}

\subsection{MLQA Zero-Shot}

We made zero-shot experiments between all the seven languages of the MLQA dataset and here are the results we got with the three models we used.

\subsubsection{mBERT}

Using mBERT, in Table \ref{MLQA_results_ZS_mBERT} we see that no matter the language of the corpus, making the question in English always gives the best score. This is probably because the dataset is trained with more data in English than the other languages. In the cases of Spanish, German, Vietnamese and Chinese, the second best score corresponds to the case where the question is asked in the language itself, but for Arabic and Hindi, we get better results when the question language is Spanish or German, instead of Arabic or Hindi.

\begin{table*}[!ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|ccccccc|c}
    \toprule
        c/q & en & es & de & ar & hi & vi & zh & avg \\ \midrule
        en & \textbf{80.3} / \textbf{67.0} & 67.4 / 52.8 & 66.4 / 52.5 & 44.1 / 31.1 & 39.3 / 26.3 & 53.7 / 39.1 & 55.8 / 41.4 & 58.1 / 44.3 \\
        es & \textbf{66.9} /\textbf{46.4} & 64.9 / 43.6 & 60.6 / 40.2 & 43.1 / 26.0 & 36.2 / 20.1 & 48.5 / 31.4 & 49.9 / 30.6 & 52.9 / 34.0 \\ 
        de & \textbf{62.4} / \textbf{46.7} & 56.4 / 41.0 & 59.4 / 43.8 & 36.8 / 23.6 & 34.0 / 21.5 & 43.6 / 29.6 & 46.5 / 30.7 & 48.4 / 33.8 \\ 
        ar & \textbf{51.1} / \textbf{33.7} & 45.4 / 28.7 & 46.3 / 30.5 & 44.9 / 28.0 & 30.8 / 17.3 & 35.9 / 20.1 & 36.8 / 21.3 & 41.6 / 25.7 \\ 
        hi & \textbf{52.9} / \textbf{37.1} & 43.7 / 29.1 & 47.6 / 33.8 & 34.5 / 21.4 & 46.2 / 30.0 & 38.0 / 25.0 & 39.2 / 25.2 & 43.2 / 28.8 \\ 
        vi & \textbf{64.5} / \textbf{44.8} & 53.9 / 37.5 & 53.7 / 36.6 & 32.5 / 19.3 & 35.1 / 19.7 & 25.8 / 39.6 & 50.3 / 32.3 & 49.8 / 32.8 \\ 
        zh & \textbf{38.3} / \textbf{37.7} & 29.0 / 28.3 & 30.0 / 28.9 & 21.0 / 20.6 & 16.6 / 16.2 & 25.1 / 24.4 & 37.4 / 36.8 & 28.2 / 27.6 \\ \midrule
        avg & 59.5 / 44.8 & 51.5 / 37.3 & 52.0 / 38.0 & 36.7 / 24.3 & 34.0 / 21.6 & 43.4 / 29.9 & 45.1 / 31.2 & 46.0 / 32.4 \\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{MLQA results (F1/EM) for each language in zero-shot with mBERT. Columns show question language, rows show context language.}
    \label{MLQA_results_ZS_mBERT}
\end{table*}


\subsubsection{XLM-R}

Using XLM-R, we see in Table \ref{MLQA_results_ZS_XLM-R} that the best scores are always those that have the same question and context language. 

\begin{table*}[!ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|ccccccc|c}
    \toprule
        c/q & en & es & de & ar & hi & vi & zh & avg \\ \midrule
        en & \textbf{80.8} / \textbf{68.0} & 57.8 / 43.9 & 60.8 / 47.1 & 33.5 / 21.3 & 45.0 / 32.0 & 39.8 / 27.5 & 37.9 / 25.3 & 50.8 / 37.9 \\ 
        es & 66.0 / 45.1 & \textbf{66.5} / \textbf{46.1} & 50.5 / 32.6 & 25.2 / 12.3 & 31.8 / 17.1 & 29.1 / 14.9 & 28.2 / 14.3 & 42.5 / 26.1 \\ 
        de & 60.0 / 44.3 & 44.0 / 29.7 & \textbf{62.2} / \textbf{46.7} & 22.2 / 12.1 & 29.4 / 17.6 & 28.7 / 16.2 & 29.1 / 17.2 & 39.4 / 26.3 \\ 
        ar & 51.5 / 33.8 & 27.0 / 13.5 & 34.2 / 19.8 & \textbf{54.6} / \textbf{36.0} & 15.6 / 5.8 & 15.0 / 5.7 & 14.1 / 5.1 & 30.3 / 17.1 \\ 
        hi & 60.6 / 43.4 & 37.4 / 23.0 & 42.8 / 27.8 & 19.5 / 8.0 & \textbf{61.4} / \textbf{44.2} & 24.3 / 11.9 & 26.1 / 13.6 & 38.9 / 24.6 \\ 
        vi & 63.6 / 44.6 & 32.6 / 19.1 & 41.9 / 25.7 & 17.8 / 6.6 & 29.2 / 15.0 & \textbf{67.2} / \textbf{46.3} & 27.4 / 13.8 & 40.0 / 24.4 \\ 
        zh & 34.9 / 34.3 & 11.3 / 10.7 & 14.0 / 13.3 & 3.9 / 3.7 & 10.8 / 10.4 & 8.1 / 7.7 & \textbf{40.0} / \textbf{39.3} & 17.6 / 17.1 \\ \midrule
        avg & 59.6 / 44.8 & 39.5 / 26.6 & 43.8 / 30.4 & 25.2 / 14.3 & 31.9 / 20.3 & 30.3 / 18.6 & 29.0 / 18.4 & 37.0 / 24.8 \\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{MLQA results (F1/EM) for each language in zero-shot with XLM-R. Columns show question language, rows show context language.}
    \label{MLQA_results_ZS_XLM-R}
\end{table*}


\subsubsection{XLM-R Large}

As we can see in Table \ref{MLQA_results_ZS_XLM-R-Large}, here also in most of the cases the best scores are when the question and the context are in the same language, even if the English scores are very close, and in some cases EM is better in English.

\begin{table*}[!ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|ccccccc|c}
    \toprule
        c/q & en & es & de & ar & hi & vi & zh & avg \\ \midrule
        en & \textbf{84.0} / \textbf{71.2} & 77.2 / 64.2 & 77.7 / 65.1 & 32.4 / 22.1 & 43.6 / 30.7 & 61.6 / 48.5 & 33.8 / 21.1 & 58.6 / 46.1 \\ 
        es & \textbf{72.1} / \textbf{50.3} & \textbf{72.1} / 50.2 & 70.0 / 48.6 & 33.0 / 17.8 & 42.1 / 26.1 & 54.8 / 35.6 & 36.5 / 20.5 & 54.4 / 35.6 \\ 
        de & 67.7 / 51.7 & 65.3 / 49.6 & \textbf{68.5} / \textbf{52.4} & 31.2 / 19.5 & 36.2 / 21.9 & 50.7 / 34.3 & 32.3 / 18.9 & 50.3 / 35.5 \\ 
        ar & 61.7 / \textbf{42.2} & 56.7 / 38.4 & 59.7 / 41.8 & \textbf{62.0} / \textbf{42.1} & 43.9 / 27.4 & 48.6 / 30.7 & 38.7 / 21.6 & 53.0 / 34.9 \\ 
        hi & 70.5 / \textbf{52.6} & 63.2 / 45.9 & 65.1 / 49.9 & 45.5 / 29.1 & \textbf{69.8} / \textbf{51.3} & 54.4 / 37.6 & 44.6 / 28.5 & 59.0 / 42.1 \\ 
        vi & 72.1 / 50.9 & 64.7 / 45.5 & 67.7 / 48.2 & 35.8 / 20.9 & 42.0 / 25.3 & \textbf{73.1} / \textbf{51.8} & 39.2 / 21.7 & 56.4 / 37.8 \\ 
        zh & 44.2 / 43.6 & 36.7 / 36.1 & 41.1 / 40.2 & 25.9 / 25.4 & 30.0 / 29.5 & 35.0 / 34.6 & \textbf{45.7} / \textbf{45.1} & 36.9 / 36.4 \\ \midrule
        avg & 67.5 / 51.8 & 62.3 / 47.1 & 64.3 / 49.5 & 38.0 / 25.3 & 43.9 / 30.3 & 54.0 / 39.0 & 38.7 / 25.3 & 52.7 / 38.3 \\ \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{MLQA results (F1/EM) for each language in zero-shot with XLM-R-Large. Columns show question language, rows show context language.}
    \label{MLQA_results_ZS_XLM-R-Large}
\end{table*}



\subsection{TyDiQA}

In Table \ref{TyDiQA_results}, we can see the results we get with TyDiQA dataset. As in the previous datasets, for zero-shot, the best results are obtained with the larger model, XLM-R large, for every language. For monolingual translate test, a large model also get the best scores, but it is BERT Large instead of RoBERTa Large. For multilingual translate test, XLM-R Large get almost every best results except for Korean en Finnish EM, where mBERT performs better. And for translate train, the results are here also balanced between Spanish and German. For fine-tuning and data augmentation, we get better results than in the rest of the experiments with this dataset, but a little lower than with XQuAD. Taking the case of Arabic, that appears both in MLQA and TyDiQA, we get better results with TyDiQA in every experiment.

\begin{table*}[!ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|ccccccccc|c}
        \toprule
        Model F1 / EM & en & ar & bn & fi & id & ko & ru & sw & te & avg \\
        \midrule
        \multicolumn{11}{l}{Zero-shot} \\
        \midrule
        mBERT & 77.8 / 69.8 & 60.6 / 45.1 & 59.5 / 47.8 & 60.6 / 50.1 & 63.0 / 50.6 & 47.9 / 39.5 & 65.3 / 47.3 & 61.0 / 49.5 & 48.9 / 41.4 & 60.5 / 49.0  \\
        XLM-R & 75.2 / 65.9 & 66.7 / 52.8 & 67.5 / 51.3 & 72.6 / 62.3 & 75.8 / 61.6 & 62.6 / 53.6 & 67.6 / 48.9 & 68.8 / 59.1 & 74.6 / 58.6 & 70.2 / 57.1  \\
        XLM-R Large & \textbf{81.4} / \textbf{70.9} & \textbf{78.2} / \textbf{64.1} & \textbf{75.3} / \textbf{60.2} & \textbf{79.8} / \textbf{68.5} & \textbf{81.7} / \textbf{68.7} & \textbf{72.9} / \textbf{62.0} & \textbf{73.1} / \textbf{52.6} & \textbf{81.6} / \textbf{71.9} & \textbf{80.8} / \textbf{67.1} & \textbf{78.3} / \textbf{65.1}\\
        \midrule
        \multicolumn{11}{l}{Translate-test monolingual} \\
        \midrule
        BERT & ~ & 67.2 / 49.8 & 76.6 / \textbf{64.1} & 71.2 / 57.5 & 74.2 / 60.3 & 70.1 / 58.9 & 71.9 / 55.5 & 76.0 / 63.8 & 63.5 / 51.1 & 71.3 / 57.6 \\
        BERT Large & ~ & \textbf{69.8} / \textbf{52.2} & \textbf{79.0} / \textbf{64.1} & \textbf{76.8} / \textbf{64.2} & \textbf{76.0} / \textbf{62.4} & \textbf{73.8} / \textbf{63.5} & \textbf{75.3} / \textbf{60.1} & \textbf{80.9} / \textbf{69.8} & \textbf{79.7} / \textbf{66.2} & \textbf{76.4} / \textbf{62.8} \\
        RoBERTa & ~ & 66.9 / 47.6 & 72.4 / 57.5 & 74.3 / 60.4 & 74.4 / 60.9 & 70.3 / 57.4 & 71.6 / 55.1 & 78.6 / 67.0 & 69.2 / 55.5 & 72.2 / 57.7 \\
        RoBERTa Large & ~ & 66.9 / 48.1 & 78.1 / 63.0 & 75.4 / 60.6 & 73.0 / 56.8 & 73.2 / 61.6 & 74.3 / 58.1 & 80.2 / 69.6 & 78.0 / 63.2 & 74.9 / 60.1\\
        \midrule
        \multicolumn{11}{l}{Translate-test multilingual} \\
        \midrule
        mBERT & ~ & 66.7 / 48.9 & 70.4 / 56.4 & 73.2 / \textbf{61.6} & 72.9 / 59.1 & \textbf{71.9} / \textbf{60.3} & 72.0 / 55.5 & 79.8 / 68.4 & 66.9 / 55.0 & 71.7 / 58.2 \\
        XLM-R & ~ & 63.6 / 46.4 & 72.4 / 60.8 & 69.6 / 56.9 & 71.2 / 58.3 & 69.6 / 56.2 & 70.8 / 55.1 & 78.8 / 68.9 & 59.4 / 46.8 & 69.4 / 56.2 \\
        XLM-R Large & ~ & \textbf{68.6} / \textbf{52.5} & \textbf{73.3} / \textbf{58.0} & \textbf{75.2} / 61.3 & \textbf{75.5} / \textbf{62.9} & 68.5 / 56.7 & \textbf{73.8} / \textbf{58.7} & \textbf{80.2} / \textbf{69.5} & \textbf{76.6} / \textbf{61.8} & \textbf{74.0} / \textbf{60.2}\\
        \midrule
        \multicolumn{11}{l}{Translate-train} \\
        \midrule
        XLM-R-es & 71.8 / 59.1 & \textbf{68.2} / \textbf{52.1} & 63.6 / 44.2 & 71.2 / 56.3 & \textbf{73.1} / 57.4 & 53.8 / 40.6 & 67.2 / 43.6 & 67.2 / 55.9 & 71.7 / \textbf{54.4} & 67.5 / 51.5 \\
        XLM-R-de & \textbf{73.6} / \textbf{63.2} & 66.0 / 50.2 & \textbf{64.7} / \textbf{49.6} & \textbf{72.4} / \textbf{60.1} & 72.4 / \textbf{60.2} & \textbf{58.2} / \textbf{44.9} & \textbf{68.2} / \textbf{51.6} & \textbf{72.1} / \textbf{63.5} & \textbf{72.8} / 53.8 & \textbf{68.9} / \textbf{55.2} \\
        \midrule
        \multicolumn{11}{l}{Fine-tuning} \\
        \midrule
        mBERT & 74.7 / 63.4 & 81.3 / 68.1 & 65.3 / 54.0 & 79.6 / 69.2 & 81.9 / 70.4 & 63.0 / 52.9 & 71.2 / 60.8 & 81.5 / 75.2 & 80.4 / 66.8 & 75.4 / 64.5  \\
        \midrule
        \multicolumn{11}{l}{Data-augmentation} \\
        \midrule
        mBERT & 84.2 / 74.1 & 86.4 / 74.7 & 77.6 / 65.5 & 84.2 / 74.2 & 88.5 / 80.2 & 75.2 / 67.4 & 81.0 / 70.1 & 85.5 / 79.8 & 84.8 / 71.6 & 83.0 / 73.0 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \centering
    \caption{TyDiQA results (F1/EM) for each language.}
    \label{TyDiQA_results}
\end{table*}


\section{Conclusions}

\bibliography{XQuAD-MLQA}
\bibliographystyle{acl_natbib}

\end{document}
